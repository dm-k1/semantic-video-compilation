{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5412d8eb",
   "metadata": {},
   "source": [
    "# Semantic Video Compilation Pipeline\n",
    "This notebook summarizes the analytical sequence and parameterization used in the study.\n",
    "\n",
    "The code cells remain to show the pipeline structure and data flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb396aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# author: Diemithry Kloppenburg\n",
    "# date: 2026-02-15\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "008eaa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================\n",
    "# Configuration\n",
    "# ============================================================\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "ROOT = Path(\"..\").resolve()\n",
    "if str(ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(ROOT))\n",
    "\n",
    "# Import step modules\n",
    "from src.steps.step1a_normalize_videos import run as step1a_normalize\n",
    "from src.steps.step1b_extract_frames import run as step1b_extract_frames\n",
    "from src.steps.step2_encode_score import run as step2_encode_and_score\n",
    "from src.steps.step3_aggregate import run as step3_aggregate\n",
    "from src.steps.step4_reweight import run as step4_reweight\n",
    "from src.steps.step5_make_strata import run as step5_make_strata\n",
    "from src.steps.step6_select_compilations import run as step6_select_compilations\n",
    "\n",
    "# Import utilities\n",
    "from src.core import set_seeds, summary\n",
    "from src.data import get_scenes\n",
    "\n",
    "# Global seed\n",
    "SEED = 1234\n",
    "\n",
    "# Output root\n",
    "OUTPUT_ROOT = Path(\"../outputs\")\n",
    "\n",
    "# ============================================================\n",
    "# Step 1a: Video Normalization\n",
    "# ============================================================\n",
    "# Use relative path to inputs/videos directory\n",
    "VIDEOS_DIR = Path(\"../inputs/videos\")\n",
    "SANITIZE_NAMES = True               # Clean filenames to ASCII\n",
    "NORMALIZE_VIDEOS = True             # Re-encode to h264/aac\n",
    "TARGET_RESOLUTION = None            # None=original, or 720, 1080, 1440, 2160\n",
    "UNIFORM_RESOLUTION = False          # True=resize all to same resolution\n",
    "FORCE_REIMPORT = False              # True=delete existing output and reimport\n",
    "\n",
    "# ============================================================\n",
    "# Step 1b: Frame Extraction\n",
    "# ============================================================\n",
    "FPS = 3                             # Frame extraction rate\n",
    "OVERWRITE_FRAMES = True\n",
    "\n",
    "# ============================================================\n",
    "# Step 2: CLIP Encoding\n",
    "# ============================================================\n",
    "CLIP_MODEL_NAME = 'ViT-B/16'\n",
    "SCENE_SOURCE = \"../inputs/custom_scenes.csv\"  # None=built-in, \"path.csv\"=from file, [\"stmt1\",...]]=custom list\n",
    "\n",
    "# ============================================================\n",
    "# Step 3: Aggregation (frame → video scores)\n",
    "# ============================================================\n",
    "AGGREGATION_METHOD = 'percentile'   # 'percentile', 'mean', 'median', 'max', 'min'\n",
    "PERCENTILE = 95                     # Only used if method='percentile'\n",
    "\n",
    "# ============================================================\n",
    "# Step 4: Representation Reweighting (center + SVD-informed weights)\n",
    "# ============================================================\n",
    "REWEIGHT_CENTERING = 'col'          # 'none', 'col', 'row', 'both'\n",
    "REWEIGHT_VARIANCE_THRESHOLD = 0.95\n",
    "REWEIGHT_K_CAP = 32\n",
    "REWEIGHT_BETA = 0.5\n",
    "\n",
    "# ============================================================\n",
    "# Step 5: Strata Construction (prototype-based grouping)\n",
    "# ============================================================\n",
    "N_STRATA = 12                       # Number of prototype strata\n",
    "K_EMBED = 16                        # SVD embedding dimension\n",
    "STRATA_INIT = \"medoid\"              # \"medoid\" or \"random\" for first prototype\n",
    "STRATA_NORMALIZE = True             # Normalize embedding rows\n",
    "\n",
    "# ============================================================\n",
    "# Step 6-7: Compilation Selection & Rendering\n",
    "# ============================================================\n",
    "N_COMPILATIONS = 10\n",
    "COMP_MIN_SECONDS = 55.0\n",
    "COMP_MAX_SECONDS = 65.0\n",
    "\n",
    "print(\"OK: Configuration loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b735b348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================\n",
    "# Step 1a: Normalize videos\n",
    "# ============================================================\n",
    "set_seeds(SEED)\n",
    "\n",
    "pipeline = step1a_normalize(\n",
    "    videos_dir=VIDEOS_DIR,\n",
    "    sanitize_names=SANITIZE_NAMES,\n",
    "    normalize=NORMALIZE_VIDEOS,\n",
    "    target_resolution=TARGET_RESOLUTION,\n",
    "    uniform_resolution=UNIFORM_RESOLUTION,\n",
    "    force_reimport=FORCE_REIMPORT,\n",
    "    output_root=OUTPUT_ROOT\n",
    ")\n",
    "\n",
    "summary(pipeline, step='import')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149e998b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================\n",
    "# Step 1b: Extract and save frames\n",
    "# ============================================================\n",
    "set_seeds(SEED)\n",
    "\n",
    "pipeline = step1b_extract_frames(\n",
    "    pipeline,\n",
    "    fps=FPS,\n",
    "    overwrite_frames=OVERWRITE_FRAMES\n",
    ")\n",
    "\n",
    "summary(pipeline, step='frames')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e06a626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================\n",
    "# Step 2: Encode frames with CLIP\n",
    "# ============================================================\n",
    "set_seeds(SEED)\n",
    "\n",
    "scenes = get_scenes(source=SCENE_SOURCE)\n",
    "print(f\"Loaded {len(scenes)} scene statements\")\n",
    "\n",
    "pipeline = step2_encode_and_score(\n",
    "    pipeline,\n",
    "    scene_statements=scenes,\n",
    "    model_name=CLIP_MODEL_NAME\n",
    ")\n",
    "\n",
    "summary(pipeline, step='encoding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a52549c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================\n",
    "# Step 3: Aggregate frame scores to video level\n",
    "# ============================================================\n",
    "set_seeds(SEED)\n",
    "\n",
    "pipeline = step3_aggregate(\n",
    "    pipeline,\n",
    "    method=AGGREGATION_METHOD,\n",
    "    percentile=PERCENTILE\n",
    ")\n",
    "\n",
    "print(\"OK: Aggregation complete (raw video scores)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28bd763",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================\n",
    "# Step 4: Representation reweighting (centering + SVD-informed weights)\n",
    "# ============================================================\n",
    "set_seeds(SEED)\n",
    "\n",
    "pipeline = step4_reweight(\n",
    "    pipeline,\n",
    "    centering=REWEIGHT_CENTERING,\n",
    "    variance_threshold=REWEIGHT_VARIANCE_THRESHOLD,\n",
    "    k_cap=REWEIGHT_K_CAP,\n",
    "    beta=REWEIGHT_BETA\n",
    ")\n",
    "\n",
    "summary(pipeline, step='aggregation')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8bae86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================\n",
    "# Visualize: Interpret SVD Components\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "\n",
    "# Get data\n",
    "video_scores = pipeline['video_scores']\n",
    "svd_scores = pipeline['svd_scores']\n",
    "scene_statements = pipeline['scene_statements']\n",
    "video_ids = pipeline['video_ids']\n",
    "\n",
    "n_components = svd_scores.shape[1]\n",
    "n_scenes = len(scene_statements)\n",
    "\n",
    "# Reconstruct component loadings by correlating original scores with SVD components\n",
    "loadings = np.zeros((n_scenes, n_components))\n",
    "for i in range(n_components):\n",
    "    for j in range(n_scenes):\n",
    "        loadings[j, i] = np.corrcoef(video_scores[:, j], svd_scores[:, i])[0, 1]\n",
    "\n",
    "# Print detailed component interpretations\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SVD COMPONENT INTERPRETATIONS\")\n",
    "print(f\"{n_components} components extracted from {n_scenes} scene statements\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "for i in range(n_components):\n",
    "    # Get top 5 scenes by absolute correlation\n",
    "    top_indices = np.argsort(np.abs(loadings[:, i]))[-5:][::-1]\n",
    "    \n",
    "    print(f\"COMPONENT {i+1}\")\n",
    "    print(f\"{'-'*80}\")\n",
    "    \n",
    "    for rank, idx in enumerate(top_indices, 1):\n",
    "        scene = scene_statements[idx]\n",
    "        corr = loadings[idx, i]\n",
    "        sign = \"positive\" if corr > 0 else \"negative\"\n",
    "        print(f\"  {rank}. [{corr:+.3f}] ({sign:>8}) {scene}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"OK: Component interpretation complete\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5041bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================\n",
    "# Step 5: Build Strata (prototype-based grouping)\n",
    "# ============================================================\n",
    "set_seeds(SEED)\n",
    "\n",
    "pipeline = step5_make_strata(\n",
    "    pipeline,\n",
    "    n_strata=\"auto\",\n",
    "    k_embed=K_EMBED,\n",
    "    init=STRATA_INIT,\n",
    "    normalize=STRATA_NORMALIZE,\n",
    "    min_stratum_size=3,\n",
    "    target_max_dist=0.35,\n",
    "    seed=SEED\n",
    ")\n",
    "\n",
    "summary(pipeline, step='strata')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51609755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================\n",
    "# Analyze Strata (prototype-based grouping)\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Get data\n",
    "embedding_Z = pipeline['embedding_Z']\n",
    "stratum_labels = pipeline['stratum_labels']\n",
    "prototype_indices = pipeline['prototype_indices']\n",
    "video_ids = pipeline['video_ids']\n",
    "n_strata = pipeline['n_strata']\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"STRATA SUMMARY\")\n",
    "print(f\"{n_strata} prototype strata covering {len(video_ids)} videos\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "\n",
    "# Compute stratum sizes and statistics\n",
    "for stratum_id in range(n_strata):\n",
    "    mask = stratum_labels == stratum_id\n",
    "    n_videos_in_stratum = mask.sum()\n",
    "    is_prototype = stratum_id in prototype_indices\n",
    "    prototype_marker = \"[PROTOTYPE]\" if is_prototype else \"\"\n",
    "    \n",
    "    # Centroid\n",
    "    stratum_centroid = embedding_Z[mask].mean(axis=0)\n",
    "    centroid_norm = np.linalg.norm(stratum_centroid)\n",
    "    \n",
    "    print(f\"Stratum {stratum_id}: {n_videos_in_stratum} videos {prototype_marker}\")\n",
    "    print(f\"  Centroid norm: {centroid_norm:.3f}\\n\")\n",
    "\n",
    "print(f\"{'='*80}\")\n",
    "print(\"OK: Strata analysis complete\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec2561d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================\n",
    "# Prototype Distances & Coverage\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Get data\n",
    "embedding_Z = pipeline['embedding_Z']\n",
    "stratum_labels = pipeline['stratum_labels']\n",
    "prototype_indices = pipeline['prototype_indices']\n",
    "video_ids = pipeline['video_ids']\n",
    "n_strata = pipeline['n_strata']\n",
    "\n",
    "# Compute distances from each video to its prototype\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "D = cosine_distances(embedding_Z, embedding_Z)\n",
    "prototype_video_embeddings = embedding_Z[prototype_indices]\n",
    "D_to_prototypes = cosine_distances(embedding_Z, prototype_video_embeddings)\n",
    "\n",
    "coverage_stats = []\n",
    "for stratum_id in range(n_strata):\n",
    "    mask = stratum_labels == stratum_id\n",
    "    distances = D_to_prototypes[mask, stratum_id]\n",
    "    coverage_stats.append({\n",
    "        'stratum_id': stratum_id,\n",
    "        'n_videos': mask.sum(),\n",
    "        'mean_dist': distances.mean(),\n",
    "        'max_dist': distances.max(),\n",
    "        'min_dist': distances.min()\n",
    "    })\n",
    "\n",
    "coverage_df = pd.DataFrame(coverage_stats)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"PROTOTYPE COVERAGE ANALYSIS\")\n",
    "print(f\"{'='*80}\\n\")\n",
    "print(coverage_df.to_string(index=False))\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Mean distance to prototype: {coverage_df['mean_dist'].mean():.4f}\")\n",
    "print(f\"Max distance to prototype: {coverage_df['max_dist'].max():.4f}\")\n",
    "print(\"OK: Coverage analysis complete\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d5b412",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================\n",
    "# Step 6: Select compilations from clusters\n",
    "# ============================================================\n",
    "set_seeds(SEED)\n",
    "\n",
    "pipeline = step6_select_compilations(\n",
    "    pipeline,\n",
    "    n_compilations=N_COMPILATIONS,\n",
    "    comp_min_seconds=COMP_MIN_SECONDS,\n",
    "    comp_max_seconds=COMP_MAX_SECONDS\n",
    ")\n",
    "\n",
    "summary(pipeline, step='compilation_selection')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07afaae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# ============================================================\n",
    "# Compilation Interchangeability Analysis\n",
    "# ============================================================\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get data\n",
    "embedding_Z = pipeline['embedding_Z']\n",
    "compilations = pipeline.get('compilations', [])\n",
    "\n",
    "if not compilations:\n",
    "    print(\"WARNING: No compilations are present in this outline.\")\n",
    "else:\n",
    "    # Get video indices for each compilation\n",
    "    compilation_indices = []\n",
    "    for comp in compilations:\n",
    "        comp_video_indices = [member['idx'] for member in comp['members']]\n",
    "        compilation_indices.append(comp_video_indices)\n",
    "\n",
    "    n_compilations = len(compilation_indices)\n",
    "    n_dimensions = embedding_Z.shape[1]\n",
    "\n",
    "    # Get unique videos across all compilations\n",
    "    all_compilation_video_indices = []\n",
    "    for indices in compilation_indices:\n",
    "        all_compilation_video_indices.extend(indices)\n",
    "    unique_video_indices = list(set(all_compilation_video_indices))\n",
    "    n_unique_videos = len(unique_video_indices)\n",
    "\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"COMPILATION INTERCHANGEABILITY ANALYSIS\")\n",
    "    print(f\"{n_compilations} compilations across {n_dimensions}-dimensional embedding space\")\n",
    "    print(f\"{n_unique_videos} unique videos across all compilations\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    print(f\"GOAL: Each compilation should be interchangeable — a representative\")\n",
    "    print(f\"      'slice' of the global library, not a special subset.\\n\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    # ========================================\n",
    "    # 1. GLOBAL REFERENCE STATISTICS\n",
    "    # ========================================\n",
    "    X = embedding_Z[unique_video_indices]\n",
    "    mu_global = X.mean(axis=0)\n",
    "    var_global = np.mean(np.sum((X - mu_global) ** 2, axis=1))\n",
    "\n",
    "    print(f\"GLOBAL REFERENCE (from {n_unique_videos} unique videos):\")\n",
    "    print(f\"  Global mean centroid: {n_dimensions}-dimensional\")\n",
    "    print(f\"  Global variance:      {var_global:.3f}\\n\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    # ========================================\n",
    "    # HELPER FUNCTIONS\n",
    "    # ========================================\n",
    "    def rbf_kernel(A, B, sigma=1.0):\n",
    "        XX = np.sum(A**2, axis=1)[:, np.newaxis]\n",
    "        YY = np.sum(B**2, axis=1)[np.newaxis, :]\n",
    "        XY = A @ B.T\n",
    "        sq_dists = XX - 2 * XY + YY\n",
    "        return np.exp(-sq_dists / (2 * sigma**2))\n",
    "\n",
    "    def compute_mmd(S, X, sigma=1.0):\n",
    "        K_SS = rbf_kernel(S, S, sigma)\n",
    "        K_XX = rbf_kernel(X, X, sigma)\n",
    "        K_SX = rbf_kernel(S, X, sigma)\n",
    "        mmd_sq = K_SS.mean() - 2 * K_SX.mean() + K_XX.mean()\n",
    "        return np.sqrt(max(0, mmd_sq))\n",
    "\n",
    "    def compute_variance_capture(S, mu_global, var_global):\n",
    "        var_s = np.mean(np.sum((S - mu_global) ** 2, axis=1))\n",
    "        return var_s / var_global if var_global > 0 else 0.0\n",
    "\n",
    "    def compute_mean_pairwise_distance(S):\n",
    "        n = len(S)\n",
    "        if n <= 1:\n",
    "            return 0.0\n",
    "        dists = []\n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                dists.append(np.linalg.norm(S[i] - S[j]))\n",
    "        return np.mean(dists)\n",
    "\n",
    "    def compute_centroid_shift(S, mu_global):\n",
    "        mu_s = S.mean(axis=0)\n",
    "        return np.linalg.norm(mu_s - mu_global)\n",
    "\n",
    "    # Auto-select sigma for MMD (median heuristic)\n",
    "    sample_size = min(100, len(X))\n",
    "    sample_indices = np.random.choice(len(X), size=sample_size, replace=False)\n",
    "    sample_X = X[sample_indices]\n",
    "    pairwise_dists = []\n",
    "    for i in range(len(sample_X)):\n",
    "        for j in range(i + 1, len(sample_X)):\n",
    "            pairwise_dists.append(np.linalg.norm(sample_X[i] - sample_X[j]))\n",
    "    sigma_mmd = np.median(pairwise_dists) if pairwise_dists else 1.0\n",
    "\n",
    "    # ========================================\n",
    "    # 2. OMNIBUS PERMUTATION TEST (PER METRIC)\n",
    "    # ========================================\n",
    "    sizes = [len(indices) for indices in compilation_indices]\n",
    "    unique_sizes = sorted(set(sizes))\n",
    "\n",
    "    B_NULL = 2000\n",
    "    B_PERM = 3000\n",
    "    EPS_STD = 1e-8\n",
    "\n",
    "    def omnibus_permutation_test(metric_name, metric_func):\n",
    "        \"\"\"\n",
    "        Omnibus test using size-matched nulls:\n",
    "        - Estimate null mean/std for each size\n",
    "        - Compute z per compilation\n",
    "        - Test T_max and T_rms via permutation\n",
    "        \"\"\"\n",
    "        # 1) Null params per size\n",
    "        null_params = {}\n",
    "        for n in unique_sizes:\n",
    "            null_vals = []\n",
    "            for _ in range(B_NULL):\n",
    "                idx = np.random.choice(len(X), size=n, replace=False)\n",
    "                R = X[idx]\n",
    "                null_vals.append(metric_func(R))\n",
    "            null_vals = np.array(null_vals)\n",
    "            mu0 = null_vals.mean()\n",
    "            sd0 = null_vals.std() + EPS_STD\n",
    "            null_params[n] = (mu0, sd0)\n",
    "\n",
    "        # 2) Observed z per compilation\n",
    "        z_obs = []\n",
    "        m_obs = []\n",
    "        for indices in compilation_indices:\n",
    "            S_c = embedding_Z[indices]\n",
    "            m_c = metric_func(S_c)\n",
    "            mu0, sd0 = null_params[len(indices)]\n",
    "            z_c = (m_c - mu0) / sd0\n",
    "            z_obs.append(z_c)\n",
    "            m_obs.append(m_c)\n",
    "        z_obs = np.array(z_obs)\n",
    "        m_obs = np.array(m_obs)\n",
    "\n",
    "        T_max_obs = np.max(np.abs(z_obs))\n",
    "        T_rms_obs = np.sqrt(np.mean(z_obs**2))\n",
    "\n",
    "        # 3) Permutation distribution for T_max and T_rms\n",
    "        T_max_null = []\n",
    "        T_rms_null = []\n",
    "        for _ in range(B_PERM):\n",
    "            z_b = []\n",
    "            for n in sizes:\n",
    "                idx = np.random.choice(len(X), size=n, replace=False)\n",
    "                R = X[idx]\n",
    "                m_b = metric_func(R)\n",
    "                mu0, sd0 = null_params[n]\n",
    "                z_b.append((m_b - mu0) / sd0)\n",
    "            z_b = np.array(z_b)\n",
    "            T_max_null.append(np.max(np.abs(z_b)))\n",
    "            T_rms_null.append(np.sqrt(np.mean(z_b**2)))\n",
    "\n",
    "        T_max_null = np.array(T_max_null)\n",
    "        T_rms_null = np.array(T_rms_null)\n",
    "\n",
    "        p_max = (1 + np.sum(T_max_null >= T_max_obs)) / (B_PERM + 1)\n",
    "        p_rms = (1 + np.sum(T_rms_null >= T_rms_obs)) / (B_PERM + 1)\n",
    "\n",
    "        # Summary stats\n",
    "        print(f\"{metric_name}:\")\n",
    "        print(f\"  Observed:   mean(m_c)={m_obs.mean():.4f}, std(m_c)={m_obs.std():.4f}\")\n",
    "        print(f\"  z-scores:   max|z|={np.max(np.abs(z_obs)):.3f}, rms(z)={np.sqrt(np.mean(z_obs**2)):.3f}\")\n",
    "        print(f\"  Omnibus p:  p_max={p_max:.4f}, p_rms={p_rms:.4f}\")\n",
    "\n",
    "        if p_max >= 0.05:\n",
    "            print(\"  OK: At α=0.05, fail to reject the null of random size-matched samples (max test).\")\n",
    "        else:\n",
    "            print(\"  WARNING: At α=0.05, reject the null of random size-matched samples (max test).\")\n",
    "\n",
    "        print()\n",
    "\n",
    "        return {\n",
    "            'metric': metric_name,\n",
    "            'p_max': p_max,\n",
    "            'p_rms': p_rms,\n",
    "            'T_max_obs': T_max_obs,\n",
    "            'T_rms_obs': T_rms_obs,\n",
    "            'z_obs': z_obs,\n",
    "            'm_obs': m_obs\n",
    "        }\n",
    "\n",
    "    print(\"OMNIBUS PERMUTATION TESTS (size-matched nulls):\\n\")\n",
    "    print(f\"Null: random sets of size n_c sampled without replacement from the {n_unique_videos}-video global pool.\")\n",
    "    print(f\"Permutations: B_NULL={B_NULL} (null parameters), B_PERM={B_PERM} (omnibus p-values).\\n\")\n",
    "\n",
    "    results = []\n",
    "    results.append(\n",
    "        omnibus_permutation_test(\n",
    "            \"Variance Capture\",\n",
    "            lambda S: compute_variance_capture(S, mu_global, var_global)\n",
    "        )\n",
    "    )\n",
    "    results.append(\n",
    "        omnibus_permutation_test(\n",
    "            \"Mean Pairwise Distance\",\n",
    "            compute_mean_pairwise_distance\n",
    "        )\n",
    "    )\n",
    "    results.append(\n",
    "        omnibus_permutation_test(\n",
    "            \"Centroid Shift\",\n",
    "            lambda S: compute_centroid_shift(S, mu_global)\n",
    "        )\n",
    "    )\n",
    "    results.append(\n",
    "        omnibus_permutation_test(\n",
    "            \"MMD to Global Pool\",\n",
    "            lambda S: compute_mmd(S, X, sigma=sigma_mmd)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # ========================================\n",
    "    # 3. FINAL STATEMENT\n",
    "    # ========================================\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"FINAL STATEMENT (α=0.05):\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "\n",
    "    for r in results:\n",
    "        verdict = \"fail to reject\" if r['p_max'] >= 0.05 else \"reject\"\n",
    "        print(\n",
    "            f\"- {r['metric']}: {verdict} the null that compilations are size-matched random samples \"\n",
    "            f\"from the {n_unique_videos}-video global pool (p_max={r['p_max']:.4f}).\"\n",
    "        )\n",
    "\n",
    "    print(\"\\nThese tests provide no evidence that any compilation set deviates from the random-sampling baseline on these metrics.\")\n",
    "    print(f\"\\n{'='*80}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a43aaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # 2. Create statistical summary figures (graphics only)\n",
    "    print(\"2. Creating statistical summary...\")\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "    fig.suptitle('Interchangeability Tests: Statistical Evidence', fontsize=14, fontweight='bold')\n",
    "    \n",
    "    metrics = [r['metric'] for r in results]\n",
    "    p_max_vals = [r['p_max'] for r in results]\n",
    "    \n",
    "    # Plot 1: p-values (top-left)\n",
    "    ax = axes[0, 0]\n",
    "    x = np.arange(len(metrics))\n",
    "    ax.bar(x, p_max_vals, alpha=0.8, color='steelblue')\n",
    "    ax.axhline(y=0.05, color='red', linestyle='--', linewidth=2, label='α=0.05')\n",
    "    ax.set_ylabel('p-value', fontsize=11)\n",
    "    ax.set_title('Omnibus p-values', fontsize=11, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics, rotation=45, ha='right', fontsize=9)\n",
    "    ax.set_ylim([0, 0.3])\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot 2: z-scores (top-right)\n",
    "    ax = axes[0, 1]\n",
    "    for i, r in enumerate(results):\n",
    "        z_scores = r['z_obs']\n",
    "        ax.scatter([i] * len(z_scores), z_scores, alpha=0.6, s=100)\n",
    "    ax.axhline(y=0, color='black', linestyle='-', linewidth=1)\n",
    "    ax.set_ylabel('z-score', fontsize=11)\n",
    "    ax.set_title('Compilation Deviations', fontsize=11, fontweight='bold')\n",
    "    ax.set_xticks(range(len(metrics)))\n",
    "    ax.set_xticklabels(metrics, rotation=45, ha='right', fontsize=9)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Plot 3: Test statistics (bottom-left)\n",
    "    ax = axes[1, 0]\n",
    "    t_max_vals = [r['T_max_obs'] for r in results]\n",
    "    ax.bar(x, t_max_vals, alpha=0.8, color='forestgreen')\n",
    "    ax.set_ylabel('T_max', fontsize=11)\n",
    "    ax.set_title('Test Statistics', fontsize=11, fontweight='bold')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(metrics, rotation=45, ha='right', fontsize=9)\n",
    "    ax.grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    # Empty (bottom-right)\n",
    "    ax = axes[1, 1]\n",
    "    ax.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../outputs/presentation_frames/02_statistical_summary.png', dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(\"OK: Statistical summary created (2x2 grid)\\n\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"WARNING: Error: {e}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
